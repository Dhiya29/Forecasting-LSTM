{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOLD STOCK FORECASTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('mae')<0.15):\n",
    "      print(\"\\nReached 15% mae so cancelling training!\")\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_B5():\n",
    "    # Ganti dengan path lokal file CSV Anda\n",
    "    file_path = 'goldstock_closing.csv'\n",
    "\n",
    "    time_step = []\n",
    "    temps = []\n",
    "\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        next(reader)  # Skip header\n",
    "        step = 0\n",
    "        for row in reader:\n",
    "            temps.append(float(row[2]))  # YOUR CODE HERE)\n",
    "            time_step.append(step)  # YOUR CODE HERE)\n",
    "            step=step + 1\n",
    "\n",
    "    series=np.array(temps) # YOUR CODE HERE\n",
    "\n",
    "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
    "    min=np.min(series)\n",
    "    max=np.max(series)\n",
    "    series -= min\n",
    "    series /= max\n",
    "    time=np.array(time_step)\n",
    "\n",
    "    # Split data\n",
    "    split_time = 1700\n",
    "    time_train = time[:split_time]\n",
    "    x_train = series[:split_time]\n",
    "    time_valid = time[split_time:]\n",
    "    x_valid = series[split_time:]\n",
    "\n",
    "    # Dataset parameters\n",
    "    window_size = 64\n",
    "    batch_size = 256\n",
    "    shuffle_buffer_size = 1000\n",
    "\n",
    "    train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "    print(train_set)\n",
    "    print(x_train.shape)\n",
    "\n",
    "    # Membangun model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n",
    "                               strides=1,\n",
    "                               activation=\"relu\",\n",
    "                               padding='causal',\n",
    "                               input_shape=[None, 1]),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    learning_rate = 2e-5\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "    callbacks = myCallback()\n",
    "    model.fit(train_set, epochs=1000, callbacks=callbacks)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('mae') < 0.11:\n",
    "            print(\"\\nReached 0.11 MAE so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)\n",
    "\n",
    "def solution_B5():\n",
    "    # Ganti dengan path lokal file CSV Anda\n",
    "    file_path = 'goldstock_closing.csv'\n",
    "    \n",
    "    time_step = []\n",
    "    temps = []\n",
    "\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        next(reader)  # Skip header\n",
    "        step = 0\n",
    "        for row in reader:\n",
    "            temps.append(float(row[2]))\n",
    "            time_step.append(step)\n",
    "            step += 1\n",
    "\n",
    "    series = np.array(temps)\n",
    "    \n",
    "    # Normalisasi data\n",
    "    min_val = np.min(series)\n",
    "    max_val = np.max(series)\n",
    "    series -= min_val\n",
    "    series /= max_val\n",
    "    time = np.array(time_step)\n",
    "\n",
    "    # Split data\n",
    "    split_time = 2000\n",
    "    time_train = time[:split_time]\n",
    "    x_train = series[:split_time]\n",
    "    time_valid = time[split_time:]\n",
    "    x_valid = series[split_time:]\n",
    "\n",
    "    # Dataset parameters\n",
    "    window_size = 64\n",
    "    batch_size = 256\n",
    "    shuffle_buffer_size = 1000\n",
    "\n",
    "    train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=\"relu\", padding='causal', input_shape=[None, 1]),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    learning_rate = 2e-5\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "    callbacks = myCallback()\n",
    "    model.fit(train_set, epochs=1000, callbacks=[callbacks])\n",
    "\n",
    "    return model, series, min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, series, min_val, max_val, time, window_size, prediction_steps):\n",
    "    forecast = []\n",
    "    input_eval = series[-window_size:]\n",
    "    for _ in range(prediction_steps):\n",
    "        input_eval = input_eval.reshape((1, -1, 1))  # Reshape untuk input model\n",
    "        prediction = model.predict(input_eval)[0, 0]  # Ambil prediksi dari array 2D\n",
    "        forecast.append(prediction)\n",
    "        input_eval = np.append(input_eval[1:], prediction)\n",
    "    # Invers normalisasi untuk hasil prediksi\n",
    "    forecast = np.array(forecast) * (max_val - min_val) + min_val\n",
    "    return forecast\n",
    "\n",
    "\n",
    "    # Jalankan fungsi dan simpan hasilnya\n",
    "    model, series, min_val, max_val, time = solution_B5()\n",
    "    \n",
    "    # Prediksi 100 hari ke depan\n",
    "    future_steps = 100\n",
    "    forecast = predict_future(model, series, min_val, max_val, time, window_size=64, prediction_steps=future_steps)\n",
    "    \n",
    "    # Plot hasil prediksi\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time, series * (max_val - min_val) + min_val, label='Historical Data')  # Convert series back to original scale\n",
    "    plt.plot(np.arange(len(series), len(series) + future_steps), forecast, label='Forecast', color='red')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Temperature')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a742a94f07b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# Jalankan fungsi dan simpan hasilnya\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution_B5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_future\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a742a94f07b2>\u001b[0m in \u001b[0;36msolution_B5\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mtemps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('mae') < 0.11:\n",
    "            print(\"\\nReached 0.11 MAE so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)\n",
    "\n",
    "def solution_B5():\n",
    "    # Ganti dengan path lokal file CSV Anda\n",
    "    file_path = 'goldstock_closing.csv'\n",
    "    \n",
    "    time_step = []\n",
    "    temps = []\n",
    "\n",
    "    with open(file_path) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        next(reader)  # Skip header\n",
    "        step = 0\n",
    "        for row in reader:\n",
    "            temps.append(float(row[2]))\n",
    "            time_step.append(step)\n",
    "            step += 1\n",
    "\n",
    "    series = np.array(temps)\n",
    "    \n",
    "    # Normalisasi data\n",
    "    min_val = np.min(series)\n",
    "    max_val = np.max(series)\n",
    "    series -= min_val\n",
    "    series /= max_val\n",
    "    time = np.array(time_step)\n",
    "\n",
    "    # Split data\n",
    "    split_time = 2000\n",
    "    time_train = time[:split_time]\n",
    "    x_train = series[:split_time]\n",
    "    time_valid = time[split_time:]\n",
    "    x_valid = series[split_time:]\n",
    "\n",
    "    # Dataset parameters\n",
    "    window_size = 64\n",
    "    batch_size = 256\n",
    "    shuffle_buffer_size = 1000\n",
    "\n",
    "    train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=\"relu\", padding='causal', input_shape=[None, 1]),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    learning_rate = 2e-5\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "    callbacks = myCallback()\n",
    "    model.fit(train_set, epochs=1000, callbacks=[callbacks])\n",
    "\n",
    "    return model, series, min_val, max_val\n",
    "\n",
    "# Jalankan fungsi dan simpan hasilnya\n",
    "model, series, min_val, max_val = solution_B5()\n",
    "\n",
    "def predict_future(model, series, window_size, prediction_steps):\n",
    "    forecast = []\n",
    "    input_eval = series[-window_size:]\n",
    "    for _ in range(prediction_steps):\n",
    "        input_eval = input_eval.reshape((1, -1, 1))  # Reshape untuk input model\n",
    "        prediction = model.predict(input_eval)[0, 0]  # Ambil prediksi dari array 2D\n",
    "        forecast.append(prediction)\n",
    "        input_eval = np.append(input_eval[0, 1:, 0], prediction)  # Update input untuk langkah berikutnya\n",
    "    return forecast\n",
    "\n",
    "# Prediksi 100 hari ke depan\n",
    "window_size = 64\n",
    "future_steps = 100\n",
    "forecast = predict_future(model, series, window_size, future_steps)\n",
    "\n",
    "# Invers normalisasi untuk hasil prediksi\n",
    "forecast = np.array(forecast) * (max_val - min_val) + min_val\n",
    "\n",
    "# Plot hasil prediksi\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, temps, label='Historical Data')\n",
    "plt.plot(np.arange(len(temps), len(temps) + future_steps), forecast, label='Forecast', color='red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8/8 [==============================] - 5s 171ms/step - loss: 0.0178 - mae: 0.1541\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.0178 - mae: 0.1538\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 1s 169ms/step - loss: 0.0177 - mae: 0.1532\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 1s 155ms/step - loss: 0.0176 - mae: 0.1526\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 0.0175 - mae: 0.1519\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 1s 155ms/step - loss: 0.0174 - mae: 0.1513\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 1s 155ms/step - loss: 0.0173 - mae: 0.1507\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 0.0172 - mae: 0.1501\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0171 - mae: 0.1495\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.0170 - mae: 0.1490\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 2s 230ms/step - loss: 0.0169 - mae: 0.1484\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0168 - mae: 0.1479\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 2s 219ms/step - loss: 0.0167 - mae: 0.1474\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 2s 231ms/step - loss: 0.0167 - mae: 0.1469\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0166 - mae: 0.1464\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.0165 - mae: 0.1460\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0164 - mae: 0.1455\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 2s 214ms/step - loss: 0.0164 - mae: 0.1451\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0163 - mae: 0.1447\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0162 - mae: 0.1442\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 2s 232ms/step - loss: 0.0162 - mae: 0.1438\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0161 - mae: 0.1434\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.0160 - mae: 0.1430\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 2s 211ms/step - loss: 0.0160 - mae: 0.1426\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.0159 - mae: 0.1422\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.0158 - mae: 0.1418\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0158 - mae: 0.1414\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 2s 249ms/step - loss: 0.0157 - mae: 0.1411\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 0.0156 - mae: 0.1407\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 0.0156 - mae: 0.1403\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.0155 - mae: 0.1399\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.0155 - mae: 0.1396\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.0154 - mae: 0.1392\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.0153 - mae: 0.1388\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0153 - mae: 0.1385\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.0152 - mae: 0.1381\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.0152 - mae: 0.1378\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0151 - mae: 0.1374\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0150 - mae: 0.1370\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0150 - mae: 0.1367\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0149 - mae: 0.1364\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0149 - mae: 0.1360\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0148 - mae: 0.1357\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0148 - mae: 0.1353\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0147 - mae: 0.1350\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0147 - mae: 0.1346\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0146 - mae: 0.1343\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0146 - mae: 0.1340\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.0145 - mae: 0.1337\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0145 - mae: 0.1333\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.0144 - mae: 0.1330\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0143 - mae: 0.1327\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0143 - mae: 0.1324\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0142 - mae: 0.1320\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 2s 212ms/step - loss: 0.0142 - mae: 0.1317\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.0141 - mae: 0.1314\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0141 - mae: 0.1311\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0141 - mae: 0.1308\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.0140 - mae: 0.1305\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 2s 199ms/step - loss: 0.0140 - mae: 0.1302\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 2s 211ms/step - loss: 0.0139 - mae: 0.1299\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0139 - mae: 0.1295\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.0138 - mae: 0.1292\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0138 - mae: 0.1289\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0137 - mae: 0.1287\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0137 - mae: 0.1284\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.0136 - mae: 0.1281\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0136 - mae: 0.1278\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 0.0136 - mae: 0.1275\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.0135 - mae: 0.1272\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0135 - mae: 0.1269\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0134 - mae: 0.1266\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0134 - mae: 0.1264\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0134 - mae: 0.1261\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 2s 223ms/step - loss: 0.0133 - mae: 0.1258\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.0133 - mae: 0.1255\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.0132 - mae: 0.1252\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.0132 - mae: 0.1250\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0132 - mae: 0.1247\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0131 - mae: 0.1244\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0131 - mae: 0.1241\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0131 - mae: 0.1239\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0130 - mae: 0.1236\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 2s 212ms/step - loss: 0.0130 - mae: 0.1233\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0129 - mae: 0.1231\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.0129 - mae: 0.1228\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0129 - mae: 0.1225\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0128 - mae: 0.1223\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0128 - mae: 0.1220\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0128 - mae: 0.1218\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0127 - mae: 0.1215\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0127 - mae: 0.1212\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0127 - mae: 0.1210\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0126 - mae: 0.1207\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.0126 - mae: 0.1205\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0126 - mae: 0.1202\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 2s 214ms/step - loss: 0.0125 - mae: 0.1200\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0125 - mae: 0.1197\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0125 - mae: 0.1195\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 2s 216ms/step - loss: 0.0124 - mae: 0.1192\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0124 - mae: 0.1189\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0124 - mae: 0.1187\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.0123 - mae: 0.1185\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0123 - mae: 0.1182\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.0123 - mae: 0.1180\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0122 - mae: 0.1177\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0122 - mae: 0.1175\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.0122 - mae: 0.1172\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0121 - mae: 0.1170\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 2s 199ms/step - loss: 0.0121 - mae: 0.1167\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 2s 199ms/step - loss: 0.0121 - mae: 0.1165\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.0120 - mae: 0.1163\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0120 - mae: 0.1160\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.0120 - mae: 0.1158\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - 2s 242ms/step - loss: 0.0119 - mae: 0.1156\n",
      "Epoch 116/1000\n",
      "8/8 [==============================] - 2s 219ms/step - loss: 0.0119 - mae: 0.1153\n",
      "Epoch 117/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0119 - mae: 0.1151\n",
      "Epoch 118/1000\n",
      "8/8 [==============================] - 2s 243ms/step - loss: 0.0118 - mae: 0.1149\n",
      "Epoch 119/1000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.0118 - mae: 0.1146\n",
      "Epoch 120/1000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0118 - mae: 0.1144\n",
      "Epoch 121/1000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.0118 - mae: 0.1142\n",
      "Epoch 122/1000\n",
      "8/8 [==============================] - 2s 213ms/step - loss: 0.0117 - mae: 0.1139\n",
      "Epoch 123/1000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.0117 - mae: 0.1137\n",
      "Epoch 124/1000\n",
      "8/8 [==============================] - 2s 212ms/step - loss: 0.0117 - mae: 0.1135\n",
      "Epoch 125/1000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.0116 - mae: 0.1132\n",
      "Epoch 126/1000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.0116 - mae: 0.1130\n",
      "Epoch 127/1000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.0116 - mae: 0.1128\n",
      "Epoch 128/1000\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0115 - mae: 0.1126\n",
      "Epoch 129/1000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0115 - mae: 0.1123\n",
      "Epoch 130/1000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.0115 - mae: 0.1121\n",
      "Epoch 131/1000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.0115 - mae: 0.1119\n",
      "Epoch 132/1000\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.0114 - mae: 0.1117\n",
      "Epoch 133/1000\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0114 - mae: 0.1114\n",
      "Epoch 134/1000\n",
      "8/8 [==============================] - 2s 212ms/step - loss: 0.0114 - mae: 0.1112\n",
      "Epoch 135/1000\n",
      "8/8 [==============================] - 2s 212ms/step - loss: 0.0113 - mae: 0.1110\n",
      "Epoch 136/1000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.0113 - mae: 0.1108\n",
      "Epoch 137/1000\n",
      "8/8 [==============================] - 2s 215ms/step - loss: 0.0113 - mae: 0.1106\n",
      "Epoch 138/1000\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.0113 - mae: 0.1104\n",
      "Epoch 139/1000\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.0112 - mae: 0.1101\n",
      "Epoch 140/1000\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0112 - mae: 0.1099\n",
      "Reached 0.11 MAE so cancelling training!\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 0.0112 - mae: 0.1099\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Sequential object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-68ce290bf876>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution_B5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Prediksi 100 hari ke depan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_future\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mforecast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable Sequential object"
     ]
    }
   ],
   "source": [
    "model, series, min_val, max_val = solution_B5()\n",
    "\n",
    "# Prediksi 100 hari ke depan\n",
    "def predict_future(model, series, window_size, prediction_steps):\n",
    "        forecast = []\n",
    "        input_eval = series[-window_size:]\n",
    "        for _ in range(prediction_steps):\n",
    "            input_eval = input_eval.reshape((1, -1, 1))  # Reshape untuk input model\n",
    "            prediction = model.predict(input_eval)[0, 0]  # Ambil prediksi dari array 2D\n",
    "            forecast.append(prediction)\n",
    "            input_eval = np.append(input_eval[1:], prediction)\n",
    "        return forecast\n",
    "\n",
    "# Prediksi 100 hari ke depan\n",
    "future_steps = 100\n",
    "forecast = predict_future(model, series, window_size, future_steps)\n",
    "    \n",
    "# Invers normalisasi untuk hasil prediksi\n",
    "forecast = np.array(forecast) * (max_val - min_val) + min_val\n",
    "    \n",
    "# Plot hasil prediksi\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, temps, label='Historical Data')\n",
    "plt.plot(np.arange(len(temps), len(temps) + future_steps), forecast, label='Forecast', color='red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('mae')<0.15):\n",
    "      print(\"\\nReached 15% mae so cancelling training!\")\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
